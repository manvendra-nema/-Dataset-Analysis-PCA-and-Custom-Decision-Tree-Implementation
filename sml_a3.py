# -*- coding: utf-8 -*-
"""SML_A3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1716lEFnXQMmlnUwFLvXM4IUgcm8mGwwq
"""



import numpy as np
from sklearn.decomposition import PCA
import random  # Add import for random module
random.seed(10)

class Node:
    def __init__(self, data_indices, depth):
        self.data_indices = data_indices
        self.depth = depth
        self.left = None
        self.right = None
        self.split_dim = None
        self.split_value = None
        self.label = None

def create_data_matrix( X_train ):
    X = X_train.reshape(-1, 784)
    X = X.T
    return X


def center_data(X, mean=None):
    if mean is None:
        mean = np.mean(X, axis=1, keepdims=True)
    X_centered = X - mean
    return X_centered, mean

def apply_pca(X_centered, p):
    covariance_matrix = np.matmul(X_centered, X_centered.T) / (X_centered.shape[1] - 1)
    V, U = np.linalg.eigh(covariance_matrix)

    sorted_indices = np.argsort(V)[::-1]
    U_sorted = U[:, sorted_indices][:, :p]

    Y = U_sorted.T @ X_centered
    return U_sorted, Y

def reconstruct_data(U_sorted, Y):
    X_recon = U_sorted @ Y
    return X_recon

def calculate_mse(X_centered, X_recon):
    mse = np.sum((X_centered - X_recon) ** 2) / X_centered.size
    return mse

def plot_reconstructed_images(X_recon_p, p):
    fig, axes = plt.subplots(10, 5, figsize=(10, 10))
    for i in range(10):
        for j in range(5):
            axes[i, j].imshow(X_recon_p[:, i * 100 + j].reshape(28, 28), cmap='cubehelix_r')
            axes[i, j].axis('off')

    plt.suptitle(f"Reconstructed Images with p={p}")
    plt.show()

def calculate_class_accuracy(y_true, y_pred):

    num_classes = 3
    accuracy_per_class = np.zeros(num_classes)
    total_per_class = np.zeros(num_classes)

    for true_label, pred_label in zip(y_true, y_pred):
        total_per_class[true_label] += 1
        if true_label == pred_label:
            accuracy_per_class[true_label] += 1

    accuracy_per_class = accuracy_per_class / total_per_class
    return accuracy_per_class



def print_tree(root, level=0, prefix="Root:"):
    if root is not None:
        print(" " * (level * 4) + prefix, root.label,root.split_dim,root.split_value)
        if root.left is not None or root.right is not None:
            if root.left is not None:
                print_tree(root.left, level + 1, prefix="L--")
            else:
                print(" " * ((level + 1) * 4) + "L--None")
            if root.right is not None:
                print_tree(root.right, level + 1, prefix="R--")
            else:
                print(" " * ((level + 1) * 4) + "R--None")



def gini_index(y):
    if len(y) == 0:
        return 0
    counts = np.bincount(y)
    probabilities = counts / len(y)
    gini = 1 - np.sum(probabilities ** 2)
    return gini


def find_best_split(X, y, data_indices):
    n_samples, n_features = X[data_indices].shape
    best_gini = float('inf')
    best_split_dim = None
    best_split_value = None
    # print(n_features)
    for dim in range(n_features):
        unique_values = np.unique(X[data_indices, dim])
        # print(unique_values)
        for i in range(len(unique_values) - 1):
            value = (unique_values[i] + unique_values[i + 1]) / 2  # Midpoint split
            left_indices = data_indices[X[data_indices, dim] <= value]
            right_indices = data_indices[X[data_indices, dim] > value]

            gini_left = gini_index(y[left_indices])
            gini_right = gini_index(y[right_indices])

            # Calculate weighted Gini impurity
            weight_left = len(left_indices) / n_samples
            weight_right = len(right_indices) / n_samples
            gini = weight_left * gini_left + weight_right * gini_right

            if gini < best_gini:
                best_gini = gini
                best_split_dim = dim
                best_split_value = value
    return best_split_dim, best_split_value




def assign_label_for_node(node, y):
    unique_labels, label_counts = np.unique(y[node.data_indices], return_counts=True)
    node.label = unique_labels[np.argmax(label_counts)]
    return node

def check_stopping_criteria(y, data_indices, total_leaf_nodes, max_leaf_nodes):
    if len(np.unique(y[data_indices])) == 1 or (max_leaf_nodes is not None and total_leaf_nodes >= max_leaf_nodes):
        return True
    return False



def grow_tree(X, y, data_indices=None, depth=0, max_depth=2):
    global total_leaf_nodes
    if data_indices is None:
        data_indices = np.arange(X.shape[0])

    n_samples, n_features = X[data_indices].shape

    node = Node(data_indices=data_indices, depth=depth)

    if depth == max_depth or check_stopping_criteria(y, data_indices, total_leaf_nodes, 2):
        assign_label_for_node(node, y)
        total_leaf_nodes += 1
        return node

    best_split_dim, best_split_value = find_best_split(X, y, data_indices)

    left_indices = data_indices[X[data_indices, best_split_dim] <= best_split_value]
    right_indices = data_indices[X[data_indices, best_split_dim] > best_split_value]

    node.split_dim = best_split_dim
    node.split_value = best_split_value

    node.left = grow_tree(X, y, left_indices, depth=depth + 1, max_depth=max_depth)
    node.right = grow_tree(X, y, right_indices, depth=depth + 1, max_depth=max_depth)

    return node

def predict(x, node):
    if node.label is not None:
        return node.label
    if x[node.split_dim] <= node.split_value:
        return predict(x, node.left)
    else:
        return predict(x, node.right)

# Load MNIST dataset
mnist_data = np.load(r"D:\Downloads\mnist.npz")
x_train, y_train = mnist_data['x_train'], mnist_data['y_train']

# Select classes 0, 1, and 2
selected_indices = np.where((y_train == 0) | (y_train == 1) | (y_train == 2))[0]
x_selected = x_train[selected_indices]
y_selected = y_train[selected_indices]


X = create_data_matrix(x_selected)
print(X.shape)
X_centered,X_train_mean = center_data(X)

p = 10

U_sorted, x_reduced = apply_pca(X_centered, p)

x_reduced = x_reduced.T
print(x_reduced.shape)



# total_leaf_nodes = 0
# tree = grow_tree(x_reduced, y_selected)


# # # Predict
# predictions = [predict(x, tree) for x in x_reduced]

# # Calculate accuracy (manually)
# accuracy = np.mean(predictions == y_selected)
# print_tree(tree)
# print("Accuracy:", accuracy)

total_leaf_nodes = 0
tree = grow_tree(x_reduced, y_selected)


# # Predict
predictions = [predict(x, tree) for x in x_reduced]

# Calculate accuracy (manually)
accuracy = np.mean(predictions == y_selected)
print_tree(tree)
print("Train Accuracy:", accuracy)

mnist_test_data = np.load(r"D:\Downloads\mnist.npz")
x_test, y_test = mnist_test_data['x_test'], mnist_test_data['y_test']


selected_test_indices = np.where((y_test == 0) | (y_test == 1) | (y_test == 2))[0]
x_test_selected = x_test[selected_test_indices]
y_test_selected = y_test[selected_test_indices]


X_test = create_data_matrix(x_test_selected)
X_test_centered, _ = center_data(X_test, mean=X_train_mean)
_, x_test_reduced = apply_pca(X_test_centered, p)
x_test_reduced = x_test_reduced.T

x_test_reduced.shape

test_predicted_labels = []
test_actual_labels = []


for test_sample, actual_label in zip(x_test_reduced, y_test_selected):

    class_counts = {0: 0, 1: 0, 2: 0}


    current_node = tree


    while current_node.label is None:
        if test_sample[current_node.split_dim] <= current_node.split_value:
            current_node = current_node.left
        else:
            current_node = current_node.right


    class_counts[current_node.label] += 1

    predicted_label = max(class_counts, key=class_counts.get)


    test_predicted_labels.append(predicted_label)
    test_actual_labels.append(actual_label)


test_accuracy = np.mean(np.array(test_predicted_labels) == np.array(test_actual_labels))


class_counts = np.bincount(y_test_selected)
class_correct_counts = np.zeros(3)

for predicted_label, actual_label in zip(test_predicted_labels, test_actual_labels):
    if predicted_label == actual_label:
        class_correct_counts[actual_label] += 1

class_accuracy = class_correct_counts / class_counts

# Print the results
print("Test Accuracy:", test_accuracy)
print("Class-wise Accuracy:", round(class_accuracy[0],2),round(class_accuracy[1],2),round(class_accuracy[2],2))



def generate_numbers_with_replacement(n):
    numbers = []
    for _ in range(n):
        numbers.append(random.randint(0, n-1))
    return numbers

def generate_bagging_bootstraped_dataset(X, y, nums):
    datasets = []
    n_samples = X.shape[0]
    for i in range(nums):
        rnd_idx = generate_numbers_with_replacement(n_samples)
        # print(rnd_idx)
        datasets.append((X[rnd_idx], y[rnd_idx]))
    return datasets


def majority_voting(predictions):
    counts = {}
    for pred in predictions:
        if pred in counts:
            counts[pred] += 1
        else:
            counts[pred] = 1
    majority_vote = max(counts, key=counts.get)
    return majority_vote

# Generate bootstrapped datasets
bagging_datasets = generate_bagging_bootstraped_dataset(x_reduced, y_selected, 5)

# Train decision trees on each bootstrapped dataset
trees = []
for i, (X_boot, y_boot) in enumerate(bagging_datasets):
    total_leaf_nodes = 0
    tree = grow_tree(X_boot, y_boot)
    trees.append(tree)
    print(f"----Dataset-{i+1}----")
    print_tree(tree)

# Predict using each decision tree and collect predictions
predictions = []
for x_sample in x_test_reduced:
    sample_predictions = [predict(x_sample, tree) for tree in trees]
    final_prediction = majority_voting(sample_predictions)
    predictions.append(final_prediction)

# Calculate accuracy for bagging
bagging_accuracy = np.mean(predictions == y_test_selected)
print("Bagging Accuracy:", bagging_accuracy)

class_counts = np.bincount(y_test_selected)
class_correct_counts = np.zeros(3)

for predicted_label, actual_label in zip(predictions, test_actual_labels):
    if predicted_label == actual_label:
        class_correct_counts[actual_label] += 1

class_accuracy = class_correct_counts / class_counts

# Print the results

print("Test Class-wise Accuracy:", round(class_accuracy[0],2),round(class_accuracy[1],2),round(class_accuracy[2],2))

# from sklearn.tree import DecisionTreeClassifier

# # Create the decision tree classifier
# tree_sklearn = DecisionTreeClassifier(criterion='gini', max_depth=2, max_leaf_nodes=3,splitter = 'best')


# # Train the decision tree on the training data
# tree_sklearn.fit(x_reduced, y_selected)

# # Predict using the trained decision tree
# predictions_sklearn = tree_sklearn.predict(x_test_reduced)

# # Calculate accuracy for the scikit-learn decision tree
# accuracy_sklearn = np.mean(predictions_sklearn == y_test_selected)
# print("Scikit-learn Decision Tree Accuracy:", accuracy_sklearn)

